# Concepts & Definitions
A concise summary of key concepts and definitions extracted from the notebooks for quick revision.

## Lib
- **UAT for NbAgg backend.**: The first line simply reloads matplotlib, uses the nbagg backend and then reloads the backend, just to ensure we have the latest modification to the backend code. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 1 - Simple figure creation using pyplot**: Should produce a figure window which is interactive with the pan and zoom buttons. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 2 - Creation of another figure, without the need to do plt.figure.**: As above, a new figure should be created. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 3 - Connection info**: The printout should show that there are two figures which have active CommSockets, and no figures pending show. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 4 - Closing figures**: Closing a specific figure instance should turn the figure into a plain image - the UI should have been removed. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 5 - No show without plt.show in non-interactive mode**: Simply doing a plt.plot should not show a new figure, nor indeed update an existing one (easily verified in UAT 6). ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 6 - Connection information**: We just created a new figure, but didn't show it. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 7 - Show of previously created figure**: We should be able to show a figure we've previously created. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 8 - Interactive mode**: In interactive mode, creating a line should result in a figure being shown. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 9 - Multiple shows**: Unlike most of the other matplotlib backends, we may want to see a figure multiple times (with or without synchronisation between the views, though the former is not yet implemented). ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 10 - Saving notebook**: Saving the notebook (with CTRL+S or File->Save) should result in the saved notebook having static versions of the figures embedded within. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 11 - Creation of a new figure on second show**: Create a figure, show it, then create a new axes and show it. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 12 - OO interface**: Should produce a new figure and plot it. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 13 - Animation**: The following should generate an animated line: ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 14 - Keyboard shortcuts in IPython after close of figure**: After closing the previous figure (with the close button above the figure) the IPython keyboard shortcuts should still function. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 15 - Figure face colours**: The nbagg honours all colours apart from that of the figure.patch. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 16 - Events**: Pressing any keyboard key or mouse button (or scrolling) should cycle the line while the figure has focus. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 17 - Timers**: Single-shot timers follow a completely different code path in the nbagg backend than regular timers (such as those used in the animation example above.)  The next set of tests ensures that both "regular" and "single-shot" timers work properly. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 18 - stopping figure when removed from DOM**: When the div that contains from the figure is removed from the DOM the figure should shut down it's comm, and if the python-side figure has no more active comms, it should destroy the figure. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))
- **UAT 19 - Blitting**: Clicking on the figure should plot a green horizontal line moving up the axes. ([notebook](ml-env/Lib/site-packages/matplotlib/backends/web_backend/nbagg_uat.ipynb))

## root
- **6. Verify Installation**: Run verification code to ensure Jupyter and all dependencies are properly installed and configured. ([notebook](setup-jupyter.ipynb))
- **5. Configure Jupyter Kernel**: Register your virtual environment as a Jupyter kernel for seamless notebook integration. ([notebook](setup-jupyter.ipynb))
- **4. Install Project Dependencies**: Install all required packages using the requirements.txt file created for this project. ([notebook](setup-jupyter.ipynb))
- **3. Set Up Virtual Environment**: Create and activate a Python virtual environment to isolate project dependencies and avoid conflicts with system packages. ([notebook](setup-jupyter.ipynb))
- **2. Create Project Directory Structure**: Organize your project with a clean directory structure for notebooks, data, and scripts. ([notebook](setup-jupyter.ipynb))
- **1. Install Jupyter Notebook**: Install Jupyter Notebook using pip package manager. ([notebook](setup-jupyter.ipynb))
- **Jupyter Notebook Setup Guide**: This notebook will guide you through setting up Jupyter for the Machine Learning Specialization project. ([notebook](setup-jupyter.ipynb))

## Linear Regression
- **Optional Lab: Model Representation**: <figure> <img src="./images/C1_W1_L3_S1_Lecture_b.png"   style="width:600px;height:200px;"> </figure> ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Goals**: In this lab you will: - Learn to implement the model $f_{w,b}$ for linear regression with one variable ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Notation**: Here is a summary of some of the notation you will encounter. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Tools**: In this lab you will make use of: - NumPy, a popular library for scientific computing - Matplotlib, a popular library for plotting data ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Problem Statement**: <img align="left" src="./images/C1_W1_L3_S1_trainingdata.png"    style=" width:380px; padding: 10px;  " /> ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Number of training examples `m`**: You will use `m` to denote the number of training examples. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Training example `x_i, y_i`**: You will use (x$^{(i)}$, y$^{(i)}$) to denote the $i^{th}$ training example. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Plotting the data** ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Model function**: <img align="left" src="./images/C1_W1_L3_S1_model.png"     style=" width:380px; padding: 10px; " > As described in lecture, the model function for linear regression (which is a function that maps from `x` to `y`) is represented as ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Challenge**: Try experimenting with different values of $w$ and $b$. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Tip:**: You can use your mouse to click on the green "Hints" below to reveal some hints for choosing b and w. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Prediction**: Now that we have a model, we can use it to make our original prediction. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Congratulations!**: In this lab you have learned: - Linear regression builds a model which establishes a relationship between features and targets - In the example above, the feature was house size and the target was house price - for simple linear regression, the model has two parameters $w$ and $b$ whose values are 'fit' using *training data*. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab02_Model_Representation_Soln.ipynb))
- **Optional Lab: Gradient Descent for Linear Regression**: <figure> <center> <img src="./images/C1_W1_L4_S1_Lecture_GD.png"  style="width:800px;height:200px;" ></center> </figure> ([notebook](supervised-learning/Linear Regression/C1_W1_Lab04_Gradient_Descent_Soln.ipynb))
- **Compute_Cost**: This was developed in the last lab. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab04_Gradient_Descent_Soln.ipynb))
- **Gradient descent summary**: So far in this course, you have developed a linear model that predicts $f_{w,b}(x^{(i)})$: $$f_{w,b}(x^{(i)}) = wx^{(i)} + b \tag{1}$$ In linear regression, you utilize input training data to fit the parameters $w$,$b$ by minimizing a measure of the error between our predictions $f_{w,b}(x^{(i)})$ and the actual data $y^{(i)}$. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab04_Gradient_Descent_Soln.ipynb))
- **Implement Gradient Descent**: You will implement gradient descent algorithm for one feature. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab04_Gradient_Descent_Soln.ipynb))
- **compute_gradient**: <a name='ex-01'></a> `compute_gradient`  implements (4) and (5) above and returns $\frac{\partial J(w,b)}{\partial w}$,$\frac{\partial J(w,b)}{\partial b}$. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab04_Gradient_Descent_Soln.ipynb))
- **Gradient Descent**: Now that gradients can be computed,  gradient descent, described in equation (3) above can be implemented below in `gradient_descent`. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab04_Gradient_Descent_Soln.ipynb))
- **Cost versus iterations of gradient descent**: A plot of cost versus iterations is a useful measure of progress in gradient descent. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab04_Gradient_Descent_Soln.ipynb))
- **Predictions**: Now that you have discovered the optimal values for the parameters $w$ and $b$, you can now use the model to predict housing values based on our learned parameters. ([notebook](supervised-learning/Linear Regression/C1_W1_Lab04_Gradient_Descent_Soln.ipynb))
- **Plotting**: You can show the progress of gradient descent during its execution by plotting the cost over iterations on a contour plot of the cost(w,b). ([notebook](supervised-learning/Linear Regression/C1_W1_Lab04_Gradient_Descent_Soln.ipynb))
- **Increased Learning Rate**: <figure> <img align="left", src="./images/C1_W1_Lab03_alpha_too_big.PNG"   style="width:340px;height:240px;" > </figure> In the lecture, there was a discussion related to the proper value of the learning rate, $\alpha$ in equation(3). ([notebook](supervised-learning/Linear Regression/C1_W1_Lab04_Gradient_Descent_Soln.ipynb))
- **Optional Lab: Python, NumPy and Vectorization**: A brief introduction to some of the scientific computing used in this course. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **Outline**: - [&nbsp;&nbsp;1.1 Goals](#toc_40015_1.1) - [&nbsp;&nbsp;1.2 Useful References](#toc_40015_1.2) - [2 Python and NumPy <a name='Python and NumPy'></a>](#toc_40015_2) - [3 Vectors](#toc_40015_3) - [&nbsp;&nbsp;3.1 Abstract](#toc_40015_3.1) - [&nbsp;&nbsp;3.2 NumPy Arrays](#toc_40015_3.2) - [&nbsp;&nbsp;3.3 Vector Creation](#toc_40015_3.3) - [&nbsp;&nbsp;3.4 Operations on Vectors](#toc_40015_3.4) - [4 Matrices](#toc_40015_4) - [&nbsp;&nbsp;4.1 Abstract](#toc_40015_4.1) - [&nbsp;&nbsp;4.2 NumPy Arrays](#toc_40015_4.2) - [&nbsp;&nbsp;4.3 Matrix Creation](#toc_40015_4.3) - [&nbsp;&nbsp;4.4 Operations on Matrices](#toc_40015_4.4) ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **1.1 Goals**: In this lab, you will: - Review the features of NumPy and Python that are used in Course 1 ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **1.2 Useful References**: - NumPy Documentation including a basic introduction: [NumPy.org](https://NumPy.org/doc/stable/) - A challenging feature topic: [NumPy Broadcasting](https://NumPy.org/doc/stable/user/basics.broadcasting.html) ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **2 Python and NumPy <a name='Python and NumPy'></a>**: Python is the programming language we will be using in this course. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3 Vectors**: <a name="toc_40015_3.1"></a> ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3.1 Abstract**: <img align="right" src="./images/C1_W2_Lab04_Vectors.PNG" style="width:340px;" >Vectors, as you will use them in this course, are ordered arrays of numbers. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3.2 NumPy Arrays**: NumPy's basic data structure is an indexable, n-dimensional *array* containing elements of the same type (`dtype`). ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3.3 Vector Creation** ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3.4 Operations on Vectors**: Let's explore some operations using vectors. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3.4.1 Indexing**: Elements of vectors can be accessed via indexing and slicing. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3.4.2 Slicing**: Slicing creates an array of indices using a set of three values (`start:stop:step`). ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3.4.3 Single vector operations**: There are a number of useful operations that involve operations on a single vector. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3.4.4 Vector Vector element-wise operations**: Most of the NumPy arithmetic, logical and comparison operations apply to vectors as well. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3.4.5 Scalar Vector operations**: Vectors can be 'scaled' by scalar values. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3.4.6 Vector Vector dot product**: The dot product is a mainstay of Linear Algebra and NumPy. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3.4.7 The Need for Speed: vector vs for loop**: We utilized the NumPy  library because it improves speed memory efficiency. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **3.4.8 Vector Vector operations in Course 1**: Vector Vector operations will appear frequently in course 1. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **4 Matrices** ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **4.1 Abstract**: Matrices, are two dimensional arrays. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **4.2 NumPy Arrays**: NumPy's basic data structure is an indexable, n-dimensional *array* containing elements of the same type (`dtype`). ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **4.3 Matrix Creation**: The same functions that created 1-D vectors will create 2-D or n-D arrays. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **4.4 Operations on Matrices**: Let's explore some operations using matrices. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **4.4.1 Indexing** ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **4.4.2 Slicing**: Slicing creates an array of indices using a set of three values (`start:stop:step`). ([notebook](supervised-learning/Linear Regression/C1_W2_Lab01_Python_Numpy_Vectorization_Soln.ipynb))
- **Optional Lab: Multiple Variable Linear Regression**: In this lab, you will extend the data structures and previously developed routines to support multiple features. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **1.2 Tools**: In this lab, we will make use of: - NumPy, a popular library for scientific computing - Matplotlib, a popular library for plotting data ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **1.3 Notation**: Here is a summary of some of the notation you will encounter, updated for multiple features. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **2 Problem Statement**: You will use the motivating example of housing price prediction. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **2.1 Matrix X containing our examples**: Similar to the table above, examples are stored in a NumPy matrix `X_train`. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **2.2 Parameter vector w, b**: * $\mathbf{w}$ is a vector with $n$ elements. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **3 Model Prediction With Multiple Variables**: The model's prediction with multiple variables is given by the linear model: ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **3.1 Single Prediction element by element**: Our previous prediction multiplied one feature value by one parameter and added a bias parameter. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **3.2 Single Prediction, vector**: Noting that equation (1) above can be implemented using the dot product as in (2) above. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **4 Compute Cost With Multiple Variables**: The equation for the cost function with multiple variables $J(\mathbf{w},b)$ is: $$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2 \tag{3}$$ where: $$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)} + b  \tag{4} $$ ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **5 Gradient Descent With Multiple Variables**: Gradient descent for multiple variables: ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **5.1 Compute Gradient with Multiple Variables**: An implementation for calculating the equations (6) and (7) is below. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **5.2 Gradient Descent With Multiple Variables**: The routine below implements equation (5) above. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **6 Congratulations!**: In this lab you: - Redeveloped the routines for linear regression, now with multiple variables. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab02_Multiple_Variable_Soln.ipynb))
- **Optional Lab: Feature scaling and Learning Rate (Multi-variable)** ([notebook](supervised-learning/Linear Regression/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb))
- **Dataset:**: | Size (sqft) | Number of Bedrooms  | Number of floors | Age of  Home | Price (1000s dollars)  | | ----------------| ------------------- |----------------- |--------------|----------------------- | | 952             | 2                   | 1                | 65           | 271.5                  | | 1244            | 3                   | 2                | 64           | 232                    | | 1947            | 3                   | 2                | 17           | 509.8                  | | ... ([notebook](supervised-learning/Linear Regression/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb))
- **Gradient Descent With Multiple Variables**: Here are the equations you developed in the last lab on gradient descent for multiple variables.: ([notebook](supervised-learning/Linear Regression/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb))
- **Learning Rate**: <figure> <img src="./images/C1_W2_Lab06_learningrate.PNG" style="width:1200px;" > </figure> The lectures discussed some of the issues related to setting the learning rate $\alpha$. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb))
- **$\alpha$ = 9.9e-7** ([notebook](supervised-learning/Linear Regression/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb))
- **$\alpha$ = 9e-7**: Let's try a bit smaller value and see what happens. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb))
- **$\alpha$ = 1e-7**: Let's try a bit smaller value for $\alpha$ and see what happens. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb))
- **Feature Scaling**: <figure> <img src="./images/C1_W2_Lab06_featurescalingheader.PNG" style="width:1200px;" > </figure> The lectures described the importance of rescaling the dataset so the features have a similar range. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb))
- **z-score normalization**: After z-score normalization, all features will have a mean of 0 and a standard deviation of 1. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb))
- **Acknowledgments**: The housing data was derived from the [Ames Housing dataset](http://jse.amstat.org/v19n3/decock.pdf) compiled by Dean De Cock for use in data science education. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab03_Feature_Scaling_and_Learning_Rate_Soln.ipynb))
- **Optional Lab: Feature Engineering and Polynomial Regression**: ![](./images/C1_W2_Lab07_FeatureEngLecture.PNG) ([notebook](supervised-learning/Linear Regression/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb))
- **Feature Engineering and Polynomial Regression Overview**: Out of the box, linear regression provides a means of building models of the form: $$f_{\mathbf{w},b} = w_0x_0 + w_1x_1+ ... ([notebook](supervised-learning/Linear Regression/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb))
- **Polynomial Features**: Above we were considering a scenario where the data was non-linear. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb))
- **Selecting Features**: <a name='GDF'></a> Above, we knew that an $x^2$ term was required. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb))
- **An Alternate View**: Above, polynomial features were chosen based on how well they matched the target data. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb))
- **Scaling features**: As described in the last lab, if the data set has features with significantly different scales, one should apply feature scaling to speed gradient descent. ([notebook](supervised-learning/Linear Regression/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb))
- **Complex Functions**: With feature engineering, even quite complex functions can be modeled: ([notebook](supervised-learning/Linear Regression/C1_W2_Lab04_FeatEng_PolyReg_Soln.ipynb))
- **Practice Lab: Linear Regression**: Welcome to your first practice lab! ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **1 - Packages**: First, let's run the cell below to import all the packages that you will need during this assignment. ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **2 -  Problem Statement**: Suppose you are the CEO of a restaurant franchise and are considering different cities for opening a new outlet. ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **3 - Dataset**: You will start by loading the dataset for this task. ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **View the variables**: Before starting on any task, it is useful to get more familiar with your dataset. ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Check the dimensions of your variables**: Another useful way to get familiar with your data is to view its dimensions. ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Visualize your data**: It is often useful to understand the data by visualizing it. ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **4 - Refresher on linear regression**: In this practice lab, you will fit the linear regression parameters $(w,b)$ to your dataset. ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **5 - Compute Cost**: Gradient descent involves repeated steps to adjust the value of your parameter $(w,b)$ to gradually get a smaller and smaller cost $J(w,b)$. ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Cost function**: As you may recall from the lecture, for one variable, the cost function for linear regression $J(w,b)$ is defined as ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Model prediction**: - For linear regression with one variable, the prediction of the model $f_{w,b}$ for an example $x^{(i)}$ is representented as: ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Implementation**: Please complete the `compute_cost()` function below to compute the cost $J(w,b)$. ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Exercise 1**: Complete the `compute_cost` below to: ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **number of training examples**: m = x.shape[0] ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **You need to return this variable correctly**: total_cost = 0 ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **START CODE HERE ###** ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Variable to keep track of sum of cost from each example**: cost_sum = 0 ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Loop over training examples**: for i in range(m): ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Your code here to get the prediction f_wb for the ith example**: f_wb = ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Your code here to get the cost associated with the ith example**: cost = ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Add to sum of cost for each example**: cost_sum = cost_sum + cost ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Get the total cost as the sum divided by (2*m)**: total_cost = (1 / (2 * m)) * cost_sum ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **END CODE HERE ###**: return total_cost ``` ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **6 - Gradient descent**: In this section, you will implement the gradient for parameters $w, b$ for linear regression. ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Exercise 2**: Please complete the `compute_gradient` function to: ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **You need to return the following variables correctly**: dj_dw = 0 dj_db = 0 ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Loop over examples**: for i in range(m): ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Your code here to get prediction f_wb for the ith example**: f_wb = ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Your code here to get the gradient for w from the ith example**: dj_dw_i = ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Your code here to get the gradient for b from the ith example**: dj_db_i = ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Update dj_db : In Python, a += 1  is the same as a = a + 1**: dj_db += dj_db_i ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Update dj_dw**: dj_dw += dj_dw_i ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **Divide both dj_dw and dj_db by m**: dj_dw = dj_dw / m dj_db = dj_db / m ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))
- **2.6 Learning parameters using batch gradient descent**: You will now find the optimal parameters of a linear regression model by using batch gradient descent. ([notebook](supervised-learning/Linear Regression/C1_W2_Linear_Regression.ipynb))

## Logistic Regression
- **Optional Lab: Classification**: In this lab, you will contrast regression and classification. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab01_Classification_Soln.ipynb))
- **Classification Problems**: <img align="left" src="./images/C1_W3_Classification.png"     style=" width:380px; padding: 10px; " > Examples of classification problems are things like: identifying email as Spam or Not Spam or determining if a tumor is malignant or benign. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab01_Classification_Soln.ipynb))
- **Linear Regression approach**: In the previous week, you applied linear regression to build a prediction model. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab01_Classification_Soln.ipynb))
- **Congratulations!**: In this lab you: - explored categorical data sets and plotting - determined that linear regression was insufficient for a classification problem. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab01_Classification_Soln.ipynb))
- **Optional Lab: Logistic Regression**: In this ungraded lab, you will - explore the sigmoid function (also known as the logistic function) - explore logistic regression; which uses the sigmoid function ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab02_Sigmoid_function_Soln.ipynb))
- **Sigmoid or Logistic Function**: <img align="left" src="./images/C1_W3_LogisticRegression_left.png"     style=" width:300px; padding: 10px; " >As discussed in the lecture videos, for a classification task, we can start by using our linear regression model, $f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot  \mathbf{x}^{(i)} + b$, to predict $y$ given $x$. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab02_Sigmoid_function_Soln.ipynb))
- **Formula for Sigmoid function**: The formula for a sigmoid function is as follows - ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab02_Sigmoid_function_Soln.ipynb))
- **Logistic Regression**: <img align="left" src="./images/C1_W3_LogisticRegression_right.png"     style=" width:300px; padding: 10px; " > A logistic regression model applies the sigmoid to the familiar linear regression model as shown below: ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab02_Sigmoid_function_Soln.ipynb))
- **Optional Lab: Logistic Regression, Decision Boundary** ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab03_Decision_Boundary_Soln.ipynb))
- **Goals**: In this lab, you will: - Plot the decision boundary for a logistic regression model. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab03_Decision_Boundary_Soln.ipynb))
- **Dataset**: Let's suppose you have following training dataset - The input variable `X` is a numpy array which has 6 training examples, each with two features - The output variable `y` is also a numpy array with 6 examples, and `y` is either `0` or `1` ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab03_Decision_Boundary_Soln.ipynb))
- **Plot data**: Let's use a helper function to plot this data. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab03_Decision_Boundary_Soln.ipynb))
- **Logistic regression model**: * Suppose you'd like to train a logistic regression model on this data which has the form ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab03_Decision_Boundary_Soln.ipynb))
- **Refresher on logistic regression and decision boundary**: * Recall that for logistic regression, the model is represented as ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab03_Decision_Boundary_Soln.ipynb))
- **Plotting decision boundary**: Now, let's go back to our example to understand how the logistic regression model is making predictions. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab03_Decision_Boundary_Soln.ipynb))
- **Optional Lab: Logistic Regression, Logistic Loss**: In this ungraded lab, you will: - explore the reason the squared error loss is not appropriate for logistic regression - explore the logistic loss function ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab04_LogisticLoss_Soln.ipynb))
- **Squared error for logistic regression?**: <img align="left" src="./images/C1_W3_SqErrorVsLogistic.png"     style=" width:400px; padding: 10px; " > Recall for **Linear** Regression we have used the **squared error cost function**: The equation for the squared error cost with one variable is: $$J(w,b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{w,b}(x^{(i)}) - y^{(i)})^2 \tag{1}$$ ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab04_LogisticLoss_Soln.ipynb))
- **Logistic Loss Function**: <img align="left" src="./images/C1_W3_LogisticLoss_a.png"     style=" width:250px; padding: 2px; " > <img align="left" src="./images/C1_W3_LogisticLoss_b.png"     style=" width:250px; padding: 2px; " > <img align="left" src="./images/C1_W3_LogisticLoss_c.png"     style=" width:250px; padding: 2px; " > ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab04_LogisticLoss_Soln.ipynb))
- **Optional Lab: Cost Function for Logistic Regression** ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab05_Cost_Function_Soln.ipynb))
- **Cost function**: In a previous lab, you developed the *logistic loss* function. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab05_Cost_Function_Soln.ipynb))
- **Code Description**: The algorithm for `compute_cost_logistic` loops over all the examples calculating the loss for each example and accumulating the total. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab05_Cost_Function_Soln.ipynb))
- **Example**: Now, let's see what the cost function output is for a different value of $w$. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab05_Cost_Function_Soln.ipynb))
- **Optional Lab: Gradient Descent for Logistic Regression** ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab06_Gradient_Descent_Soln.ipynb))
- **Data set**: Let's start with the same two feature data set used in the decision boundary lab. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab06_Gradient_Descent_Soln.ipynb))
- **Logistic Gradient Descent**: <img align="right" src="./images/C1_W3_Logistic_gradient_descent.png"     style=" width:400px; padding: 10px; " > ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab06_Gradient_Descent_Soln.ipynb))
- **Gradient Descent Implementation**: The gradient descent algorithm implementation has two components: - The loop implementing equation (1) above. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab06_Gradient_Descent_Soln.ipynb))
- **Calculating the Gradient, Code Description**: Implements equation (2),(3) above for all $w_j$ and $b$. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab06_Gradient_Descent_Soln.ipynb))
- **Gradient Descent Code**: The code implementing equation (1) above is implemented below. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab06_Gradient_Descent_Soln.ipynb))
- **Let's plot the results of gradient descent:** ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab06_Gradient_Descent_Soln.ipynb))
- **Another Data set**: Let's return to a one-variable data set. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab06_Gradient_Descent_Soln.ipynb))
- **Ungraded Lab:  Logistic Regression using Scikit-Learn** ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab07_Scikit_Learn_Soln.ipynb))
- **Fit the model**: The code below imports the [logistic regression model](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html#sklearn.linear_model.LogisticRegression) from scikit-learn. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab07_Scikit_Learn_Soln.ipynb))
- **Make Predictions**: You can see the predictions made by this model by calling the `predict` function. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab07_Scikit_Learn_Soln.ipynb))
- **Calculate accuracy**: You can calculate this accuracy of this model by calling the `score` function. ([notebook](supervised-learning/Logistic Regression/C1_W3_Lab07_Scikit_Learn_Soln.ipynb))
- **Outline**: - [ 1 - Packages ](#1) - [ 2 - Logistic Regression](#2) - [ 2.1 Problem Statement](#2.1) - [ 2.2 Loading and visualizing the data](#2.2) - [ 2.3  Sigmoid function](#2.3) - [ 2.4 Cost function for logistic regression](#2.4) - [ 2.5 Gradient for logistic regression](#2.5) - [ 2.6 Learning parameters using gradient descent ](#2.6) - [ 2.7 Plotting the decision boundary](#2.7) - [ 2.8 Evaluating logistic regression](#2.8) - [ 3 - Regularized Logistic Regression](#3) - [ 3.1 Problem Statement](#3.1) - [ 3.2 Loading and visualizing the data](#3.2) - [ 3.3 Feature mapping](#3.3) - [ 3.4 Cost function for regularized logistic regression](#3.4) - [ 3.5 Gradient for regularized logistic regression](#3.5) - [ 3.6 Learning parameters using gradient descent](#3.6) - [ 3.7 Plotting the decision boundary](#3.7) - [ 3.8 Evaluating regularized logistic regression model](#3.8) ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **1 - Packages**: First, let's run the cells below to import all the packages that you will need during this assignment. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **2 - Logistic Regression**: In this part of the exercise, you will build a logistic regression model to predict whether a student gets admitted into a university. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **2.1 Problem Statement**: Suppose that you are the administrator of a university department and you want to determine each applicantâ€™s chance of admission based on their results on two exams. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **2.2 Loading and visualizing the data**: You will start by loading the dataset for this task. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **View the variables**: Let's get more familiar with your dataset. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Check the dimensions of your variables**: Another useful way to get familiar with your data is to view its dimensions. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Visualize your data**: Before starting to implement any learning algorithm, it is always good to visualize the data if possible. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **2.3  Sigmoid function**: Recall that for logistic regression, the model is represented as ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Exercise 1**: Please complete  the `sigmoid` function to calculate ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **2.4 Cost function for logistic regression**: In this section, you will implement the cost function for logistic regression. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Exercise 2**: Please complete the `compute_cost` function using the equations below. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **START CODE HERE ###**: loss_sum = 0 ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Loop over each training example**: for i in range(m): ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **First calculate z_wb = w[0]*X[i][0]+...+w[n-1]*X[i][n-1]+b**: z_wb = 0 ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Loop over each feature**: for j in range(n): ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Add the corresponding term to z_wb**: z_wb_ij = # Your code here to calculate w[j] * X[i][j] z_wb += z_wb_ij # equivalent to z_wb = z_wb + z_wb_ij ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Add the bias term to z_wb**: z_wb += b # equivalent to z_wb = z_wb + b ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **END CODE HERE ###**: return total_cost ``` <br> ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **2.5 Gradient for logistic regression**: In this section, you will implement the gradient for logistic regression. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Exercise 3**: Please complete the `compute_gradient` function to compute $\frac{\partial J(\mathbf{w},b)}{\partial w}$, $\frac{\partial J(\mathbf{w},b)}{\partial b}$ from equations (2) and (3) below. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Calculate f_wb (exactly as you did in the compute_cost function above)**: f_wb = ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Calculate the  gradient for b from this example**: dj_db_i = # Your code here to calculate the error ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **add that to dj_db**: dj_db += dj_db_i ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **get dj_dw for each attribute**: for j in range(n): ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **You code here to calculate the gradient from the i-th example for j-th attribute**: dj_dw_ij = dj_dw[j] += dj_dw_ij ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **divide dj_db and dj_dw by total number of examples**: dj_dw = dj_dw / m dj_db = dj_db / m ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Calculate f_wb (exactly how you did it in the compute_cost function above)**: z_wb = 0 ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Add bias term**: z_wb += b ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Calculate the prediction from the model**: f_wb = sigmoid(z_wb) </details> ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **2.6 Learning parameters using gradient descent**: Similar to the previous assignment, you will now find the optimal parameters of a logistic regression model by using gradient descent. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **With the following settings**: np.random.seed(1) initial_w = 0.01 * (np.random.rand(2) - 0.5) initial_b = -8 iterations = 10000 alpha = 0.001 ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **2.7 Plotting the decision boundary**: We will now use the final parameters from gradient descent to plot the linear fit. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **2.8 Evaluating logistic regression**: We can evaluate the quality of the parameters we have found by seeing how well the learned model predicts on our training set. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Exercise 4**: Please complete the `predict` function to produce `1` or `0` predictions given a dataset and a learned parameter vector $w$ and $b$. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **number of training examples**: m, n = X.shape p = np.zeros(m) ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Loop over each example**: for i in range(m): ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **using a couple of lines of code**: f_wb = ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Calculate the prediction for that training example**: p[i] = # Your code here to calculate the prediction based on f_wb ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **3 - Regularized Logistic Regression**: In this part of the exercise, you will implement regularized logistic regression to predict whether microchips from a fabrication plant passes quality assurance (QA). ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **3.1 Problem Statement**: Suppose you are the product manager of the factory and you have the test results for some microchips on two different tests. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **3.2 Loading and visualizing the data**: Similar to previous parts of this exercise, let's start by loading the dataset for this task and visualizing it. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **3.3 Feature mapping**: One way to fit the data better is to create more features from each data point. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **3.4 Cost function for regularized logistic regression**: In this part, you will implement the cost function for regularized logistic regression. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Exercise 5**: Please complete the `compute_cost_reg` function below to calculate the following term for each element in $w$ $$\frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2$$ ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Calls the compute_cost function that you implemented above**: cost_without_reg = compute_cost(X, y, w, b) ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **You need to calculate this value**: reg_cost = 0. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Add the regularization cost to get the total cost**: total_cost = cost_without_reg + reg_cost ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **3.5 Gradient for regularized logistic regression**: In this section, you will implement the gradient for regularized logistic regression. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Exercise 6**: Please complete the `compute_gradient_reg` function below to modify the code below to calculate the following term ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Loop over the elements of w**: for j in range(n): ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Add the regularization term  to the correspoding element of dj_dw**: dj_dw[j] = dj_dw[j] + dj_dw_j_reg ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **3.6 Learning parameters using gradient descent**: Similar to the previous parts, you will use your gradient descent function implemented above to learn the optimal parameters $w$,$b$. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **Using the following settings** ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **np.random.seed(1)** ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **initial_w = np.random.rand(X_mapped.shape[1])-0.5** ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **initial_b = 1.** ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **lambda_ = 0.01;** ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **iterations = 10000** ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **alpha = 0.01**: Iteration    0: Cost     0.72 Iteration 1000: Cost     0.59 Iteration 2000: Cost     0.56 Iteration 3000: Cost     0.53 Iteration 4000: Cost     0.51 Iteration 5000: Cost     0.50 Iteration 6000: Cost     0.48 Iteration 7000: Cost     0.47 Iteration 8000: Cost     0.46 Iteration 9000: Cost     0.45 Iteration 9999: Cost     0.45 ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **3.7 Plotting the decision boundary**: To help you visualize the model learned by this classifier, we will use our `plot_decision_boundary` function which plots the (non-linear) decision boundary that separates the positive and negative examples. ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))
- **3.8 Evaluating regularized logistic regression model**: You will use the `predict` function that you implemented above to calculate the accuracy of the regularized logistic regression model on the training set ([notebook](supervised-learning/Logistic Regression/C1_W3_Logistic_Regression.ipynb))

## Overfitting
- **Ungraded Lab:  Overfitting**: <img align="left" src="./images/C1_W3_Overfitting_a.png"     style=" width:250px; padding: 10px; " > <img align="left" src="./images/C1_W3_Overfitting_b.png"     style=" width:250px; padding: 10px; " > <img align="left" src="./images/C1_W3_Overfitting_c.png"     style=" width:250px; padding: 10px; " > ([notebook](supervised-learning/Overfitting/C1_W3_Lab08_Overfitting_Soln.ipynb))
- **Goals**: In this lab, you will explore: - the situations where overfitting can occur - some of the solutions ([notebook](supervised-learning/Overfitting/C1_W3_Lab08_Overfitting_Soln.ipynb))
- **Overfitting**: The week's lecture described situations where overfitting can arise. ([notebook](supervised-learning/Overfitting/C1_W3_Lab08_Overfitting_Soln.ipynb))
- **Congratulations!**: You have developed some intuition about the causes and solutions to overfitting. ([notebook](supervised-learning/Overfitting/C1_W3_Lab08_Overfitting_Soln.ipynb))
- **Optional Lab - Regularized Cost and Gradient** ([notebook](supervised-learning/Overfitting/C1_W3_Lab09_Regularization_Soln.ipynb))
- **Adding regularization**: <img align="Left" src="./images/C1_W3_LinearGradientRegularized.png"  style=" width:400px; padding: 10px; " > <img align="Center" src="./images/C1_W3_LogisticGradientRegularized.png"  style=" width:400px; padding: 10px; " > ([notebook](supervised-learning/Overfitting/C1_W3_Lab09_Regularization_Soln.ipynb))
- **Cost functions with regularization** ([notebook](supervised-learning/Overfitting/C1_W3_Lab09_Regularization_Soln.ipynb))
- **Cost function for regularized linear regression**: The equation for the cost function regularized linear regression is: $$J(\mathbf{w},b) = \frac{1}{2m} \sum\limits_{i = 0}^{m-1} (f_{\mathbf{w},b}(\mathbf{x}^{(i)}) - y^{(i)})^2  + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2 \tag{1}$$ where: $$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = \mathbf{w} \cdot \mathbf{x}^{(i)} + b  \tag{2} $$ ([notebook](supervised-learning/Overfitting/C1_W3_Lab09_Regularization_Soln.ipynb))
- **Cost function for regularized logistic regression**: For regularized **logistic** regression, the cost function is of the form $$J(\mathbf{w},b) = \frac{1}{m}  \sum_{i=0}^{m-1} \left[ -y^{(i)} \log\left(f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) - \left( 1 - y^{(i)}\right) \log \left( 1 - f_{\mathbf{w},b}\left( \mathbf{x}^{(i)} \right) \right) \right] + \frac{\lambda}{2m}  \sum_{j=0}^{n-1} w_j^2 \tag{3}$$ where: $$ f_{\mathbf{w},b}(\mathbf{x}^{(i)}) = sigmoid(\mathbf{w} \cdot \mathbf{x}^{(i)} + b)  \tag{4} $$ ([notebook](supervised-learning/Overfitting/C1_W3_Lab09_Regularization_Soln.ipynb))
- **Gradient descent with regularization**: The basic algorithm for running gradient descent does not change with regularization, it is: $$\begin{align*} &\text{repeat until convergence:} \; \lbrace \\ &  \; \; \;w_j = w_j -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial w_j} \tag{1}  \; & \text{for j := 0..n-1} \\ &  \; \; \;  \; \;b = b -  \alpha \frac{\partial J(\mathbf{w},b)}{\partial b} \\ &\rbrace \end{align*}$$ Where each iteration performs simultaneous updates on $w_j$ for all $j$. ([notebook](supervised-learning/Overfitting/C1_W3_Lab09_Regularization_Soln.ipynb))
- **Computing the Gradient with regularization (both linear/logistic)**: The gradient calculation for both linear and logistic regression are nearly identical, differing only in computation of $f_{\mathbf{w}b}$. ([notebook](supervised-learning/Overfitting/C1_W3_Lab09_Regularization_Soln.ipynb))
- **Gradient function for regularized linear regression** ([notebook](supervised-learning/Overfitting/C1_W3_Lab09_Regularization_Soln.ipynb))
- **Gradient function for regularized logistic regression** ([notebook](supervised-learning/Overfitting/C1_W3_Lab09_Regularization_Soln.ipynb))
- **Rerun over-fitting example** ([notebook](supervised-learning/Overfitting/C1_W3_Lab09_Regularization_Soln.ipynb))
